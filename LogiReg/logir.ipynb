{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 102)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:102\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"hi)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "import warnings \n",
    "warnings.filterwarnings( \"ignore\" ) \n",
    "\n",
    "# to compare our model's accuracy with sklearn model \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "# Logistic Regression \n",
    "class LogitRegression() : \n",
    "\tdef __init__( self, learning_rate, iterations ) :\t\t \n",
    "\t\tself.learning_rate = learning_rate\t\t \n",
    "\t\tself.iterations = iterations \n",
    "\t\t\n",
    "\t# Function for model training\t \n",
    "\tdef fit( self, X, Y ) :\t\t \n",
    "\t\t# no_of_training_examples, no_of_features\t\t \n",
    "\t\tself.m, self.n = X.shape\t\t \n",
    "\t\t# weight initialization\t\t \n",
    "\t\tself.W = np.zeros( self.n )\t\t \n",
    "\t\tself.b = 0\t\t\n",
    "\t\tself.X = X\t\t \n",
    "\t\tself.Y = Y \n",
    "\t\t\n",
    "\t\t# gradient descent learning \n",
    "\t\t\t\t\n",
    "\t\tfor i in range( self.iterations ) :\t\t\t \n",
    "\t\t\tself.update_weights()\t\t\t \n",
    "\t\treturn self\n",
    "\t\n",
    "\t# Helper function to update weights in gradient descent \n",
    "\t\n",
    "\tdef update_weights( self ) :\t\t \n",
    "\t\tA = 1 / ( 1 + np.exp( - ( self.X.dot( self.W ) + self.b ) ) ) \n",
    "\t\t\n",
    "\t\t# calculate gradients\t\t \n",
    "\t\ttmp = ( A - self.Y.T )\t\t \n",
    "\t\ttmp = np.reshape( tmp, self.m )\t\t \n",
    "\t\tdW = np.dot( self.X.T, tmp ) / self.m\t\t \n",
    "\t\tdb = np.sum( tmp ) / self.m \n",
    "\t\t\n",
    "\t\t# update weights\t \n",
    "\t\tself.W = self.W - self.learning_rate * dW\t \n",
    "\t\tself.b = self.b - self.learning_rate * db \n",
    "\t\t\n",
    "\t\treturn self\n",
    "\t\n",
    "\t# Hypothetical function h( x ) \n",
    "\t\n",
    "\tdef predict( self, X ) :\t \n",
    "\t\tZ = 1 / ( 1 + np.exp( - ( X.dot( self.W ) + self.b ) ) )\t\t \n",
    "\t\tY = np.where( Z > 0.5, 1, 0 )\t\t \n",
    "\t\treturn Y \n",
    "\n",
    "\n",
    "# Driver code \n",
    "\n",
    "def main() : \n",
    "\t\n",
    "\t# Importing dataset\t \n",
    "\tdf = pd.read_csv( \"diabetes.csv\" ) \n",
    "\tX = df.iloc[:,:-1].values \n",
    "\tY = df.iloc[:,-1:].values \n",
    "\t\n",
    "\t# Splitting dataset into train and test set \n",
    "\tX_train, X_test, Y_train, Y_test = train_test_split( \n",
    "\tX, Y, test_size = 1/3, random_state = 0 ) \n",
    "\t\n",
    "\t# Model training\t \n",
    "\tmodel = LogitRegression( learning_rate = 0.01, iterations = 1000 ) \n",
    "\t\n",
    "\tmodel.fit( X_train, Y_train )\t \n",
    "\tmodel1 = LogisticRegression()\t \n",
    "\tmodel1.fit( X_train, Y_train) \n",
    "\t\n",
    "\t# Prediction on test set \n",
    "\tY_pred = model.predict( X_test )\t \n",
    "\tY_pred1 = model1.predict( X_test ) \n",
    "\t\n",
    "\t# measure performance\t \n",
    "\tcorrectly_classified = 0\t\n",
    "\tcorrectly_classified1 = 0\n",
    "\t\n",
    "\t# counter\t \n",
    "\tcount = 0\t\n",
    "\tfor count in range( np.size( Y_pred ) ) : \n",
    "\t\t\n",
    "\t\tif Y_test[count] == Y_pred[count] :\t\t\t \n",
    "\t\t\tcorrectly_classified = correctly_classified + 1\n",
    "\t\t\n",
    "\t\tif Y_test[count] == Y_pred1[count] :\t\t\t \n",
    "\t\t\tcorrectly_classified1 = correctly_classified1 + 1\n",
    "\t\t\t\n",
    "\t\tcount = count + 1\n",
    "\tnew_data_entry = np.array([[6, 148, 72, 35, 0, 33.6, 0.627, 50]])\n",
    "\tprediction_custom_model = model.predict(new_data_entry)\n",
    "\tprediction_sklearn_model = model1.predict(new_data_entry)\n",
    "\tprint(\"Predicting for [6, 148, 72, 35, 0, 33.6, 0.627, 50]\")\n",
    "\tprint(\"Prediction by our model for the new data entry:\",prediction_custom_model)\n",
    "\tprint(\"Prediction by sklearn model for the new data entry:\",prediction_sklearn_model)\n",
    "\tprint(\"Actual outcome:1\")\n",
    "\tprint(\"\\n\")\n",
    "\tprint( \"Accuracy on test set by our model\t : \", ( \n",
    "\tcorrectly_classified / count ) * 100 ) \n",
    "\tprint( \"Accuracy on test set by sklearn model : \", ( \n",
    "\tcorrectly_classified1 / count ) * 100 ) \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\t \n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
